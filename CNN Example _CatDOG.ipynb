{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal: Create a model that can classify cat and dog images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing --- Goal is to make your data compatible for CNN !!!\n",
    "# Tensorflow offers direct classes to achieve the same\n",
    "\n",
    "# Image Generators\n",
    "# rescale= 1.0 / 255.  ensures my intensities of each pixel will be in the range of 0 - 255 (Image Normalization)\n",
    "\n",
    "train_generator = tf.keras.preprocessing.image.ImageDataGenerator(  rescale= 1.0 / 255. ,\n",
    "                                                                    rotation_range=30,\n",
    "                                                                    zoom_range=0.2,\n",
    "                                                                    horizontal_flip=True, \n",
    "                                                                    fill_mode='nearest' )\n",
    "test_generator = tf.keras.preprocessing.image.ImageDataGenerator( rescale= 1.0 / 255. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#Image Gen for Cat\n",
    "imageGen = train_generator.flow_from_directory(\"catDataset/\",\n",
    "                         batch_size=1,\n",
    "                         save_to_dir=\"outputCatDog/cat/\", #Ensure the directory exists else prog will throw error\n",
    "                         save_prefix=\"cat-\",\n",
    "                         save_format=\"jpg\")\n",
    "counter = 0\n",
    "for generatedImage in imageGen:\n",
    "    \n",
    "    counter += 1 \n",
    "    \n",
    "    if counter == 250:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#Image Gen for dog\n",
    "imageGen = train_generator.flow_from_directory(\"dogDataset/\",\n",
    "                         batch_size=1,\n",
    "                         save_to_dir=\"outputCatDog/dog/\", #Ensure the directory exists else prog will throw error\n",
    "                         save_prefix=\"dog-\",\n",
    "                         save_format=\"jpg\")\n",
    "counter = 0\n",
    "for generatedImage in imageGen:\n",
    "    \n",
    "    counter += 1 \n",
    "    \n",
    "    if counter == 250:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2502 images belonging to 3 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Pass the images through generator to generate compatible inputs\n",
    "train_generator1 = tf.keras.preprocessing.image.ImageDataGenerator(  rescale= 1.0 / 255.\n",
    "                                                                  )\n",
    "test_generator1 = tf.keras.preprocessing.image.ImageDataGenerator( rescale= 1.0 / 255. )\n",
    "\n",
    "trainImageData = train_generator1.flow_from_directory('cats_and_dogs/train' ,\n",
    "                                                    batch_size=20,\n",
    "                                                    class_mode='binary',\n",
    "                                                    target_size=(64,64)) \n",
    "\n",
    "\n",
    "testImageData = train_generator1.flow_from_directory('cats_and_dogs/validation' ,\n",
    "                                                    batch_size=20,\n",
    "                                                    class_mode='binary',\n",
    "                                                    target_size=(64,64)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Architect our CNN\n",
    "trainImageData.image_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "#Conv2D(noFeatureMaps , kernelShape, input_shape, activation)\n",
    "\n",
    "\n",
    "#Step1: Create First Convolutional Layer\n",
    "#Convolve Layer\n",
    "model.add(tf.keras.layers.Conv2D(32 , (3,3) , input_shape = (64, 64, 3) , activation = 'relu' , padding='same'))\n",
    "#Pooling Layer\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size = (2,2) ))\n",
    "#model.add(tf.keras.layers.AveragePooling2D(pool_size = (2,2) ))\n",
    "\n",
    "#Step2: Create Second Convolutional Layer\n",
    "#Convolve Layer\n",
    "model.add(tf.keras.layers.Conv2D(16 , (3,3) , activation = 'relu' , padding='same'))\n",
    "#Pooling Layer\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size = (2,2) ))\n",
    "\n",
    "#Step3: Flatten\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "#Step4:\n",
    "\n",
    "model.add(tf.keras.layers.Dense(units= 512 , activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dense(units= 256 , activation = 'relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(units= 1 , activation = 'sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 64, 64, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 16)        4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               2097664   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 2,234,769\n",
      "Trainable params: 2,234,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile\n",
    "model.compile(optimizer=\"adam\",\n",
    "             loss=\"binary_crossentropy\",\n",
    "              metrics=['accuracy']\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(len(trainImageData.filenames) / trainImageData.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 125 steps, validate for 50 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 28s 223ms/step - loss: -11352584.1682 - accuracy: 0.5002 - val_loss: 91315709.2000 - val_accuracy: 0.5000\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 23s 187ms/step - loss: -2208105845.9252 - accuracy: 0.4978 - val_loss: 8159859425.2800 - val_accuracy: 0.5000\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 24s 190ms/step - loss: -37271500189.2609 - accuracy: 0.5006 - val_loss: 90599693189.1200 - val_accuracy: 0.5000\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 24s 191ms/step - loss: -238126686831.2588 - accuracy: 0.5002 - val_loss: 459773519462.4000 - val_accuracy: 0.5000\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 24s 189ms/step - loss: -915315611781.5826 - accuracy: 0.4998 - val_loss: 1522134417408.0000 - val_accuracy: 0.5000\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 24s 189ms/step - loss: -2605010545980.0059 - accuracy: 0.4994 - val_loss: 3923705515212.7998 - val_accuracy: 0.5000\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 24s 192ms/step - loss: -6110005114278.2031 - accuracy: 0.4994 - val_loss: 8601810554060.7998 - val_accuracy: 0.5000\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 24s 191ms/step - loss: -12442791201578.8105 - accuracy: 0.4998 - val_loss: 16739920301260.8008 - val_accuracy: 0.5000\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 24s 192ms/step - loss: -22810646463221.5469 - accuracy: 0.5002 - val_loss: 29487723739873.2812 - val_accuracy: 0.5000\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 24s 191ms/step - loss: -39070747194737.2734 - accuracy: 0.4998 - val_loss: 49024880041000.9609 - val_accuracy: 0.5000\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 24s 193ms/step - loss: -62336701653917.5234 - accuracy: 0.5010 - val_loss: 76242843777105.9219 - val_accuracy: 0.5000\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 24s 189ms/step - loss: -95387973571245.4531 - accuracy: 0.4998 - val_loss: 114111066522255.3594 - val_accuracy: 0.5000\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - 24s 191ms/step - loss: -139349901879909.9062 - accuracy: 0.5006 - val_loss: 164658386221137.9062 - val_accuracy: 0.5000\n",
      "Epoch 14/50\n",
      "125/125 [==============================] - 24s 193ms/step - loss: -198091834202807.8438 - accuracy: 0.4998 - val_loss: 229836861063823.3750 - val_accuracy: 0.5000\n",
      "Epoch 15/50\n",
      "125/125 [==============================] - 23s 185ms/step - loss: -271603372148767.7812 - accuracy: 0.5018 - val_loss: 311992587659509.7500 - val_accuracy: 0.5000\n",
      "Epoch 16/50\n",
      "125/125 [==============================] - 24s 194ms/step - loss: -366634510130245.3125 - accuracy: 0.4990 - val_loss: 414785645534248.9375 - val_accuracy: 0.5000\n",
      "Epoch 17/50\n",
      "125/125 [==============================] - 24s 191ms/step - loss: -478252378095117.8750 - accuracy: 0.5022 - val_loss: 541045210847641.6250 - val_accuracy: 0.5000\n",
      "Epoch 18/50\n",
      "125/125 [==============================] - 24s 193ms/step - loss: -620083381831260.7500 - accuracy: 0.4998 - val_loss: 690273010015273.0000 - val_accuracy: 0.5000\n",
      "Epoch 19/50\n",
      "125/125 [==============================] - 24s 190ms/step - loss: -791609841939796.7500 - accuracy: 0.4982 - val_loss: 872902828228608.0000 - val_accuracy: 0.5000\n",
      "Epoch 20/50\n",
      "125/125 [==============================] - 24s 189ms/step - loss: -986151308174912.5000 - accuracy: 0.5006 - val_loss: 1084256298540728.3750 - val_accuracy: 0.5000\n",
      "Epoch 21/50\n",
      "125/125 [==============================] - 24s 190ms/step - loss: -1216321522166706.0000 - accuracy: 0.5006 - val_loss: 1331788657396285.5000 - val_accuracy: 0.5000\n",
      "Epoch 22/50\n",
      "125/125 [==============================] - 24s 190ms/step - loss: -1483682100746953.5000 - accuracy: 0.5010 - val_loss: 1613749608314306.5000 - val_accuracy: 0.5000\n",
      "Epoch 23/50\n",
      "125/125 [==============================] - 24s 189ms/step - loss: -1793616720341225.2500 - accuracy: 0.5006 - val_loss: 1939389481400402.0000 - val_accuracy: 0.5000\n",
      "Epoch 24/50\n",
      "125/125 [==============================] - 24s 190ms/step - loss: -2143673712383211.2500 - accuracy: 0.5010 - val_loss: 2307453997222461.5000 - val_accuracy: 0.5000\n",
      "Epoch 25/50\n",
      "125/125 [==============================] - 24s 190ms/step - loss: -2537714422504279.5000 - accuracy: 0.5006 - val_loss: 2729667882312007.5000 - val_accuracy: 0.5000\n",
      "Epoch 26/50\n",
      "125/125 [==============================] - 24s 194ms/step - loss: -2999618555518813.5000 - accuracy: 0.4998 - val_loss: 3201136189285335.0000 - val_accuracy: 0.5000\n",
      "Epoch 27/50\n",
      "125/125 [==============================] - 23s 186ms/step - loss: -3495743916235380.0000 - accuracy: 0.5014 - val_loss: 3725917195159470.0000 - val_accuracy: 0.5000\n",
      "Epoch 28/50\n",
      "125/125 [==============================] - 23s 183ms/step - loss: -4077201870997082.5000 - accuracy: 0.4986 - val_loss: 4315665889618821.0000 - val_accuracy: 0.5000\n",
      "Epoch 29/50\n",
      "125/125 [==============================] - 23s 183ms/step - loss: -4685676724795338.0000 - accuracy: 0.5006 - val_loss: 4960307959792927.0000 - val_accuracy: 0.5000\n",
      "Epoch 30/50\n",
      "125/125 [==============================] - 23s 181ms/step - loss: -5372130183857832.0000 - accuracy: 0.5004 - val_loss: 5703005435246674.0000 - val_accuracy: 0.5000\n",
      "Epoch 31/50\n",
      "125/125 [==============================] - 23s 183ms/step - loss: -6171238368205495.0000 - accuracy: 0.4986 - val_loss: 6473337047036723.0000 - val_accuracy: 0.5000\n",
      "Epoch 32/50\n",
      "125/125 [==============================] - 23s 186ms/step - loss: -6999778747745063.0000 - accuracy: 0.5002 - val_loss: 7347047442530959.0000 - val_accuracy: 0.5000\n",
      "Epoch 33/50\n",
      "125/125 [==============================] - 23s 184ms/step - loss: -7902942581170361.0000 - accuracy: 0.5006 - val_loss: 8295323957104476.0000 - val_accuracy: 0.5000\n",
      "Epoch 34/50\n",
      "125/125 [==============================] - 23s 187ms/step - loss: -8911003934587833.0000 - accuracy: 0.5010 - val_loss: 9353147392133694.0000 - val_accuracy: 0.5000\n",
      "Epoch 35/50\n",
      "125/125 [==============================] - 23s 186ms/step - loss: -10018556420810738.0000 - accuracy: 0.4990 - val_loss: 10424651226787348.0000 - val_accuracy: 0.5000\n",
      "Epoch 36/50\n",
      "125/125 [==============================] - 24s 189ms/step - loss: -11192001506307294.0000 - accuracy: 0.4990 - val_loss: 11645888831244206.0000 - val_accuracy: 0.5000\n",
      "Epoch 37/50\n",
      "125/125 [==============================] - 24s 190ms/step - loss: -12547317941509728.0000 - accuracy: 0.4982 - val_loss: 12981173150443110.0000 - val_accuracy: 0.5000\n",
      "Epoch 38/50\n",
      "125/125 [==============================] - 23s 187ms/step - loss: -13863948952268896.0000 - accuracy: 0.4998 - val_loss: 14412035760586752.0000 - val_accuracy: 0.5000\n",
      "Epoch 39/50\n",
      "125/125 [==============================] - 24s 192ms/step - loss: -15387790895305330.0000 - accuracy: 0.4986 - val_loss: 15921233388036424.0000 - val_accuracy: 0.5000\n",
      "Epoch 40/50\n",
      "125/125 [==============================] - 24s 188ms/step - loss: -16972572421583060.0000 - accuracy: 0.5002 - val_loss: 17560090462482596.0000 - val_accuracy: 0.5000\n",
      "Epoch 41/50\n",
      "125/125 [==============================] - 24s 194ms/step - loss: -18707066533338004.0000 - accuracy: 0.5002 - val_loss: 19306708277122828.0000 - val_accuracy: 0.5000\n",
      "Epoch 42/50\n",
      "125/125 [==============================] - 24s 188ms/step - loss: -20553812309193076.0000 - accuracy: 0.4998 - val_loss: 21271724942541456.0000 - val_accuracy: 0.5000\n",
      "Epoch 43/50\n",
      "125/125 [==============================] - 23s 188ms/step - loss: -22534819260651600.0000 - accuracy: 0.4998 - val_loss: 23212040778594384.0000 - val_accuracy: 0.5000\n",
      "Epoch 44/50\n",
      "125/125 [==============================] - 24s 194ms/step - loss: -24639315189558484.0000 - accuracy: 0.4998 - val_loss: 25453492749048216.0000 - val_accuracy: 0.5000\n",
      "Epoch 45/50\n",
      "125/125 [==============================] - 24s 194ms/step - loss: -26864772595714784.0000 - accuracy: 0.4998 - val_loss: 27679417875856096.0000 - val_accuracy: 0.5000\n",
      "Epoch 46/50\n",
      "125/125 [==============================] - 24s 190ms/step - loss: -29199830579174080.0000 - accuracy: 0.5010 - val_loss: 30053143779485940.0000 - val_accuracy: 0.5000\n",
      "Epoch 47/50\n",
      "125/125 [==============================] - 24s 189ms/step - loss: -31817196889427896.0000 - accuracy: 0.4990 - val_loss: 32785860315483996.0000 - val_accuracy: 0.5000\n",
      "Epoch 48/50\n",
      "125/125 [==============================] - 24s 189ms/step - loss: -34505516060589772.0000 - accuracy: 0.5002 - val_loss: 35541737978021808.0000 - val_accuracy: 0.5000\n",
      "Epoch 49/50\n",
      "125/125 [==============================] - 23s 187ms/step - loss: -37438204416569656.0000 - accuracy: 0.4986 - val_loss: 38337650533964184.0000 - val_accuracy: 0.5000\n",
      "Epoch 50/50\n",
      "125/125 [==============================] - 24s 188ms/step - loss: -40409712640030512.0000 - accuracy: 0.4998 - val_loss: 41491306120174632.0000 - val_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "#Fit\n",
    "\n",
    "history = model.fit(trainImageData,\n",
    "                   validation_data=testImageData,\n",
    "                   epochs=50,\n",
    "                   steps_per_epoch= int(len(trainImageData.filenames) / trainImageData.batch_size), #No of Images / batch size\n",
    "                   validation_steps= int(len(testImageData.filenames) / testImageData.batch_size)\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 125 steps, validate for 50 steps\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 24s 193ms/step - loss: -5711676.9332 - accuracy: 0.4966 - val_loss: 44259080.0400 - val_accuracy: 0.5000\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 24s 192ms/step - loss: -1001648784.8986 - accuracy: 0.5010 - val_loss: 3629669329.9200 - val_accuracy: 0.5000\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 23s 184ms/step - loss: -16390015536.6006 - accuracy: 0.5010 - val_loss: 39957151539.2000 - val_accuracy: 0.5000\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 24s 190ms/step - loss: -104469821730.6388 - accuracy: 0.5006 - val_loss: 202764119572.4800 - val_accuracy: 0.5000\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 24s 191ms/step - loss: -404338362053.3112 - accuracy: 0.5002 - val_loss: 674495630540.8000 - val_accuracy: 0.5000\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 24s 190ms/step - loss: -1149069443399.4641 - accuracy: 0.5006 - val_loss: 1750323941867.5200 - val_accuracy: 0.5000\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 23s 185ms/step - loss: -2680726416363.5269 - accuracy: 0.4998 - val_loss: 3795442424872.9600 - val_accuracy: 0.5000\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 23s 188ms/step - loss: -5496635716760.0771 - accuracy: 0.4998 - val_loss: 7364776747335.6797 - val_accuracy: 0.5000\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 24s 190ms/step - loss: -10228333901154.8027 - accuracy: 0.4978 - val_loss: 13160288658391.0391 - val_accuracy: 0.5000\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 23s 185ms/step - loss: -17278456205797.3535 - accuracy: 0.5022 - val_loss: 21696520273264.6406 - val_accuracy: 0.5000\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 23s 186ms/step - loss: -27666320641471.8164 - accuracy: 0.5018 - val_loss: 33848700971253.7617 - val_accuracy: 0.5000\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 24s 189ms/step - loss: -42408869579705.7891 - accuracy: 0.4990 - val_loss: 50708195500359.6797 - val_accuracy: 0.5000\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - 24s 189ms/step - loss: -62234795456170.1562 - accuracy: 0.4986 - val_loss: 73356879090155.5156 - val_accuracy: 0.5000\n",
      "Epoch 14/50\n",
      "125/125 [==============================] - 24s 194ms/step - loss: -87822244834018.9531 - accuracy: 0.5002 - val_loss: 102043840759726.0781 - val_accuracy: 0.5000\n",
      "Epoch 15/50\n",
      "125/125 [==============================] - 23s 188ms/step - loss: -120350162487233.3281 - accuracy: 0.5014 - val_loss: 138471406064107.5156 - val_accuracy: 0.5000\n",
      "Epoch 16/50\n",
      "125/125 [==============================] - 24s 188ms/step - loss: -161840348403026.2812 - accuracy: 0.5018 - val_loss: 184192826270023.6875 - val_accuracy: 0.5000\n",
      "Epoch 17/50\n",
      "125/125 [==============================] - 24s 189ms/step - loss: -212491766740518.5000 - accuracy: 0.5006 - val_loss: 239167621981798.4062 - val_accuracy: 0.5000\n",
      "Epoch 18/50\n",
      "125/125 [==============================] - 23s 186ms/step - loss: -275354462539326.6562 - accuracy: 0.4994 - val_loss: 306451267494871.0625 - val_accuracy: 0.5000\n",
      "Epoch 19/50\n",
      "125/125 [==============================] - 24s 190ms/step - loss: -349078289644248.0625 - accuracy: 0.5006 - val_loss: 386031204559749.1250 - val_accuracy: 0.5000\n",
      "Epoch 20/50\n",
      "125/125 [==============================] - 24s 189ms/step - loss: -437567412817906.5000 - accuracy: 0.4998 - val_loss: 479177439738593.2500 - val_accuracy: 0.5000\n",
      "Epoch 21/50\n",
      "125/125 [==============================] - 23s 187ms/step - loss: -538487481692720.5625 - accuracy: 0.5002 - val_loss: 588498411360092.1250 - val_accuracy: 0.5000\n",
      "Epoch 22/50\n",
      "125/125 [==============================] - 23s 188ms/step - loss: -657971582446126.1250 - accuracy: 0.5006 - val_loss: 715271610134691.8750 - val_accuracy: 0.5000\n",
      "Epoch 23/50\n",
      "125/125 [==============================] - 23s 187ms/step - loss: -794053116743317.7500 - accuracy: 0.4998 - val_loss: 858735646406082.5000 - val_accuracy: 0.5000\n",
      "Epoch 24/50\n",
      "125/125 [==============================] - 23s 185ms/step - loss: -951136624021716.5000 - accuracy: 0.5006 - val_loss: 1020170003011338.2500 - val_accuracy: 0.5000\n",
      "Epoch 25/50\n",
      "125/125 [==============================] - 23s 185ms/step - loss: -1128888262978786.0000 - accuracy: 0.5002 - val_loss: 1209163593736519.7500 - val_accuracy: 0.5000\n",
      "Epoch 26/50\n",
      "125/125 [==============================] - 23s 181ms/step - loss: -1328322376518738.0000 - accuracy: 0.4998 - val_loss: 1417625737027911.7500 - val_accuracy: 0.5000\n",
      "Epoch 27/50\n",
      "125/125 [==============================] - 23s 183ms/step - loss: -1550068001814857.2500 - accuracy: 0.4998 - val_loss: 1649338420025098.2500 - val_accuracy: 0.5000\n",
      "Epoch 28/50\n",
      "125/125 [==============================] - 23s 184ms/step - loss: -1797876805134275.2500 - accuracy: 0.5006 - val_loss: 1912014825513287.7500 - val_accuracy: 0.5000\n",
      "Epoch 29/50\n",
      "125/125 [==============================] - 24s 189ms/step - loss: -2079383145011064.5000 - accuracy: 0.5006 - val_loss: 2199840013376553.0000 - val_accuracy: 0.5000\n",
      "Epoch 30/50\n",
      "125/125 [==============================] - 24s 191ms/step - loss: -2386254522926278.0000 - accuracy: 0.5002 - val_loss: 2521619518854267.0000 - val_accuracy: 0.5000\n",
      "Epoch 31/50\n",
      "125/125 [==============================] - 23s 186ms/step - loss: -2714196597846799.5000 - accuracy: 0.5006 - val_loss: 2861650165733785.5000 - val_accuracy: 0.5000\n",
      "Epoch 32/50\n",
      "125/125 [==============================] - 23s 186ms/step - loss: -3082849097182845.5000 - accuracy: 0.5022 - val_loss: 3244417530357350.5000 - val_accuracy: 0.5000\n",
      "Epoch 33/50\n",
      "125/125 [==============================] - 24s 190ms/step - loss: -3486324769672834.5000 - accuracy: 0.5010 - val_loss: 3671684492789022.5000 - val_accuracy: 0.5000\n",
      "Epoch 34/50\n",
      "125/125 [==============================] - 24s 189ms/step - loss: -3938849045367825.5000 - accuracy: 0.5010 - val_loss: 4119814151486832.5000 - val_accuracy: 0.5000\n",
      "Epoch 35/50\n",
      "125/125 [==============================] - 24s 194ms/step - loss: -4423358597562494.0000 - accuracy: 0.5002 - val_loss: 4621185309265101.0000 - val_accuracy: 0.5000\n",
      "Epoch 36/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: -4927920084682818.0000 - accuracy: 0.5022"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "#Conv2D(noFeatureMaps , kernelShape, input_shape, activation)\n",
    "\n",
    "\n",
    "#Step1: Create First Convolutional Layer\n",
    "#Convolve Layer\n",
    "model.add(tf.keras.layers.Conv2D(16 , (3,3) , input_shape = (64, 64, 3) , activation = 'relu' , padding='same'))\n",
    "#Pooling Layer\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size = (2,2) ))\n",
    "#model.add(tf.keras.layers.AveragePooling2D(pool_size = (2,2) ))\n",
    "\n",
    "#Step2: Create Second Convolutional Layer\n",
    "#Convolve Layer\n",
    "model.add(tf.keras.layers.Conv2D(16 , (3,3) , activation = 'relu' , padding='same'))\n",
    "#Pooling Layer\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size = (2,2) ))\n",
    "\n",
    "#Step3: Flatten\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "#Step4:\n",
    "\n",
    "model.add(tf.keras.layers.Dense(units= 512 , activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dense(units= 256 , activation = 'relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(units= 1 , activation = 'sigmoid'))\n",
    "\n",
    "#Compile\n",
    "model.compile(optimizer=\"adam\",\n",
    "             loss=\"binary_crossentropy\",\n",
    "              metrics=['accuracy']\n",
    "             )\n",
    "\n",
    "\n",
    "history2 = model.fit(trainImageData,\n",
    "                   validation_data=testImageData,\n",
    "                   epochs=50,\n",
    "                   steps_per_epoch= int(len(trainImageData.filenames) / trainImageData.batch_size), #No of Images / batch size\n",
    "                   validation_steps= int(len(testImageData.filenames) / testImageData.batch_size)\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "100/100 [==============================] - 4s 36ms/step - loss: 0.7040 - accuracy: 0.5765 - val_loss: 0.6757 - val_accuracy: 0.5980\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 3s 35ms/step - loss: 0.6535 - accuracy: 0.6190 - val_loss: 0.6244 - val_accuracy: 0.6560\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.5616 - accuracy: 0.7000 - val_loss: 0.6273 - val_accuracy: 0.6730\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.4736 - accuracy: 0.7775 - val_loss: 0.6762 - val_accuracy: 0.6580\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.3667 - accuracy: 0.8300 - val_loss: 0.6500 - val_accuracy: 0.6970\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 3s 35ms/step - loss: 0.2548 - accuracy: 0.8945 - val_loss: 0.8056 - val_accuracy: 0.6470\n",
      "Epoch 7/50\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.1850 - accuracy: 0.9340 - val_loss: 1.1080 - val_accuracy: 0.6110\n",
      "Epoch 8/50\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.1186 - accuracy: 0.9595 - val_loss: 1.1964 - val_accuracy: 0.6280\n",
      "Epoch 9/50\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.0914 - accuracy: 0.9695 - val_loss: 1.2382 - val_accuracy: 0.6510\n",
      "Epoch 10/50\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.0585 - accuracy: 0.9810 - val_loss: 1.3183 - val_accuracy: 0.6750\n",
      "Epoch 11/50\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.0495 - accuracy: 0.9855 - val_loss: 1.2598 - val_accuracy: 0.6830\n",
      "Epoch 12/50\n",
      "100/100 [==============================] - 3s 35ms/step - loss: 0.0293 - accuracy: 0.9925 - val_loss: 1.4395 - val_accuracy: 0.6540\n",
      "Epoch 13/50\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.0583 - accuracy: 0.9765 - val_loss: 1.3667 - val_accuracy: 0.6730\n",
      "Epoch 14/50\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.0262 - accuracy: 0.9920 - val_loss: 1.6934 - val_accuracy: 0.6630\n",
      "Epoch 15/50\n",
      "100/100 [==============================] - 3s 35ms/step - loss: 0.0260 - accuracy: 0.9885 - val_loss: 1.5793 - val_accuracy: 0.6530\n",
      "Epoch 16/50\n",
      "100/100 [==============================] - 4s 37ms/step - loss: 0.0240 - accuracy: 0.9925 - val_loss: 1.5853 - val_accuracy: 0.6400\n",
      "Epoch 17/50\n",
      " 73/100 [====================>.........] - ETA: 0s - loss: 0.0099 - accuracy: 0.9966"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-5f870c2199ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m                    \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m                    \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainImageData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtrainImageData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m#No of Images / batch size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m                    \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestImageData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtestImageData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m                    )\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pnDlMinEnv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pnDlMinEnv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m               \u001b[1;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m               \u001b[1;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pnDlMinEnv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pnDlMinEnv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    609\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pnDlMinEnv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2420\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2422\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pnDlMinEnv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1665\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1667\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pnDlMinEnv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pnDlMinEnv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pnDlMinEnv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "#Conv2D(noFeatureMaps , kernelShape, input_shape, activation)\n",
    "\n",
    "\n",
    "#Step1: Create First Convolutional Layer\n",
    "#Convolve Layer\n",
    "model.add(tf.keras.layers.Conv2D(16 , (3,3) , input_shape = (64, 64, 3) , activation = 'tanh' , padding='same'))\n",
    "#Pooling Layer\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size = (2,2) ))\n",
    "#model.add(tf.keras.layers.AveragePooling2D(pool_size = (2,2) ))\n",
    "\n",
    "#Step2: Create Second Convolutional Layer\n",
    "#Convolve Layer\n",
    "model.add(tf.keras.layers.Conv2D(16 , (3,3) , activation = 'tanh' , padding='same'))\n",
    "#Pooling Layer\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size = (2,2) ))\n",
    "\n",
    "#Step3: Flatten\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "#Step4:\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(units= 512 , activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dense(units= 256 , activation = 'relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(units= 1 , activation = 'sigmoid'))\n",
    "\n",
    "#Compile\n",
    "model.compile(optimizer=tf.keras.optimizers.Nadam(),\n",
    "             loss=\"binary_crossentropy\",\n",
    "              metrics=['accuracy']\n",
    "             )\n",
    "\n",
    "\n",
    "history2 = model.fit(trainImageData,\n",
    "                   validation_data=testImageData,\n",
    "                   epochs=50,\n",
    "                   steps_per_epoch= int(len(trainImageData.filenames) / trainImageData.batch_size), #No of Images / batch size\n",
    "                   validation_steps= int(len(testImageData.filenames) / testImageData.batch_size)\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the model to classify the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x222732a3c50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22276f68898>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = tf.keras.preprocessing.image.load_img('cats_and_dogs/train/cats/cat.103.jpg' , target_size=(64,64))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "imgArray = tf.keras.preprocessing.image.img_to_array(img)\n",
    "\n",
    "imgArray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 64, 64, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "compatibleImgArray = np.expand_dims(imgArray, axis=0)\n",
    "\n",
    "compatibleImgArray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-28-f3d8dcac21bf>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(compatibleImgArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainImageData.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its a Cat\n"
     ]
    }
   ],
   "source": [
    "if model.predict_classes(compatibleImgArray) == 0:\n",
    "    print(\"Its a Cat\")\n",
    "else:\n",
    "    print(\"Its a Dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
